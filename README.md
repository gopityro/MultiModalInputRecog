# Multi Model Input System
This project is a basic demo that shows how to use hand gestures and voice commands to interact with a system. While the idea was to control the mouse using gestures, this version does not include real mouse control yet.

Built with Python, it uses MediaPipe to track hand gestures, Tkinter for the interface, and the speech_recognition library to take voice commands. Both input methods work together, showing visual feedback to help users understand whatâ€™s happening.

The goal is to explore multimodal interaction, letting people use gestures and voice instead of just a keyboard or mouse. This can be useful in hands-free situations and is inspired by modern AI systems that understand speech and images.

`Run main.py to see the out of the project`:

<img width="692" height="480" alt="Picture1" src="https://github.com/user-attachments/assets/b5b01541-5b97-47e5-8a7c-a6effd6c880f" />
